{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Assignment 1: Pose classification with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     Mihai Blaga (1085020) Kai Stevens-Noguchi (963632)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to **two** questions of your choosing.\n",
    "\n",
    "If you are in a group of 2, you will respond to **four** questions of your choosing.\n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer should be submitted separately as a PDF."
   ]
  },
  {
   "source": [
    "### Naive Bayes classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "TEST_LOCATION = \".\\\\COMP30027_2021_assignment1_data\\\\test.csv\"\n",
    "TRAIN_LOCATION = \".\\\\COMP30027_2021_assignment1_data\\\\train.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "\n",
    "#input location of the data\n",
    "\n",
    "def preprocess(csv_location: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes CSV location as input and outputs a Dataframe with null values converted to NaN and appropriate column names.\n",
    "    Specify specific column: data[column_name]\n",
    "    Specify specific row: data.loc[row_no]\n",
    "    Specify specific element: data.loc[row_no, column_name]\n",
    "    \"\"\"\n",
    "    columns = ['label', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9','y10', 'y11']\n",
    "    # Change 9999 to np.nan when reading in the data\n",
    "    data = pd.read_csv(csv_location, header = None, na_values = 9999, names = columns) \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should calculate prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "#input dataframe of training data\n",
    "#output dataframe with mean, datagframe with std and prior probabilities for each label\n",
    "\n",
    "def train(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, dict]:\n",
    "     \"\"\"\n",
    "     It take a pandas dataframe and returns 2 pandas DataFrame with mean and standard deviation of each features/attributes for each labels/class.\n",
    "     example output of mean_df:\n",
    "          |  x1  |  x2  |  x3  | ...\n",
    "     label| 2.31   21.4   3.21\n",
    "     label| 31.2   10.9   45.2 \n",
    "     label| \n",
    "     \"\"\"\n",
    "     # groupby each label and compute the mean and std for each label\n",
    "     grouped = data.groupby(\"label\")\n",
    "     mean_df: pd.DataFrame = grouped.agg(np.mean)\n",
    "     std_df: pd.DataFrame = grouped.agg(np.std)\n",
    "     # calculate logged prior probabilities\n",
    "     total_size: int = len(data.index)\n",
    "     group_sizes: pd.Series = grouped.size()\n",
    "     probabilities: dict = {label: np.log(group_sizes[label] / total_size) for label in group_sizes.index}\n",
    "     return (mean_df, std_df, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set)\n",
    "\n",
    "#input Dataframe and train output\n",
    "#output list of labels\n",
    "\n",
    "def predict(train_df: pd.DataFrame, test_df: pd.DataFrame, use_missing=False) -> list:\n",
    "    \"\"\"\n",
    "    Takes train data and test data as input, create naive bayes classifier and reture a list that contains predicted labels of test. \n",
    "    When use_missing is set to true, implementation for Q5 is used.\n",
    "    \"\"\"\n",
    "    if use_missing:\n",
    "        missing_probabilities = calculate_missing(train_df)\n",
    "\n",
    "    # get mean, std and prior probabilities\n",
    "    mean_df, std_df, prior_probabilities = train(train_df)\n",
    "    prediction: list = list()\n",
    "\n",
    "    # iterate over all points in test set and find the probability of occurance\n",
    "    for _, row in test_df.iterrows():\n",
    "        probabilities = prior_probabilities.copy()\n",
    "        for feature, value in row.iloc[1:].iteritems():\n",
    "            if np.isnan(value) and not use_missing:\n",
    "                continue\n",
    "\n",
    "            for label in prior_probabilities.keys():\n",
    "                # if a point is missing use probability of point missing\n",
    "                if np.isnan(value):\n",
    "                    likelihood = missing_probabilities[feature][label]\n",
    "                else:\n",
    "                    likelihood = scipy.stats.norm(mean_df[feature][label], std_df[feature][label]).pdf(value)\n",
    "                # if probability is zero change with epsilon\n",
    "                if likelihood == 0:\n",
    "                    likelihood = sys.float_info.epsilon\n",
    "                probabilities[label] += np.log(likelihood)\n",
    "\n",
    "        # pick the label with highest probability\n",
    "        prediction.append(max(probabilities.items(), key=lambda x: x[1])[0])\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "#theoretically run predict(test.csv) and compare that to test[\"labels\"]\n",
    "#input list of labels and list of true values\n",
    "#output score and individual label score.\n",
    "\n",
    "def evaluate(prediction: list, ground_truth: list) -> Tuple[dict, pd.DataFrame]: \n",
    "    data = pd.DataFrame({\"ground truth\": ground_truth, \"prediction\": prediction})\n",
    "    \n",
    "    num_inst = len(data)\n",
    "    num_classes = len(set(data[\"ground truth\"].values))\n",
    "\n",
    "    #initial counters\n",
    "    micro_tp = 0\n",
    "    micro_fp = 0\n",
    "    micro_fn = 0\n",
    "\n",
    "    macro_prec = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "\n",
    "    wa_prec = 0\n",
    "    wa_recall = 0\n",
    "    wa_f1 = 0\n",
    "\n",
    "    output = pd.DataFrame(columns = [\"precision\", \"recall\", \"f1\", \"num_label\"])\n",
    "    indexes = []\n",
    "\n",
    "    for label in set(data[\"ground truth\"].values):\n",
    "        tp = len(data.loc[(data[\"ground truth\"] == label) & (data[\"prediction\"] == label)])\n",
    "        fp = len(data.loc[(data[\"ground truth\"] != label) & (data[\"prediction\"] == label)])\n",
    "        fn = len(data.loc[(data[\"ground truth\"] == label) & (data[\"prediction\"] != label)])\n",
    "        num_label = len(data.loc[data[\"ground truth\"] == label])\n",
    "\n",
    "        micro_tp = micro_tp + tp\n",
    "        micro_fp = micro_fp + fp\n",
    "        micro_fn = micro_fn + fn\n",
    "\n",
    "        #conditionals to avoid division by 0 errors.\n",
    "        if (tp > 0):\n",
    "            prec = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "        else:\n",
    "            prec = 0\n",
    "            recall = 0\n",
    "\n",
    "        if (prec + recall > 0):\n",
    "            f1 = 2*prec*recall / (prec + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "\n",
    "        #macro calculations\n",
    "        macro_prec = macro_prec + prec\n",
    "        macro_recall = macro_recall + recall\n",
    "        macro_f1 = macro_f1 + f1\n",
    "\n",
    "        #weighted average calculations\n",
    "        wa_prec = wa_prec + (num_label/num_inst)*prec\n",
    "        wa_recall = wa_recall + (num_label/num_inst)*recall\n",
    "        wa_f1 = wa_f1 + (num_label/num_inst)*f1\n",
    "\n",
    "        #for broken down calculations of precision, recall and f1\n",
    "        temp = pd.DataFrame([[prec, recall, f1, num_label]], columns = [\"precision\", \"recall\", \"f1\", \"num_label\"])\n",
    "        indexes = indexes + [label]\n",
    "        output = output.append(temp)\n",
    "\n",
    "    #final macro division\n",
    "    macro_prec = macro_prec / num_classes\n",
    "    macro_recall = macro_recall / num_classes\n",
    "    macro_f1 = macro_f1 / num_classes\n",
    "\n",
    "    #micro calculations\n",
    "    micro_prec = micro_tp / (micro_tp + micro_fp)\n",
    "    micro_recall = micro_tp / (micro_tp + micro_fn)\n",
    "    micro_f1 = 2*micro_prec*micro_recall / (micro_prec + micro_recall)\n",
    "\n",
    "    #table of broken down calculations used by Q1\n",
    "    output.index = indexes\n",
    "    # print(\"Output table for Q1\")\n",
    "    print(output.sort_values(\"precision\"))\n",
    "\n",
    "    scores = {\"macro precision\": macro_prec, \"macro recall\": macro_recall, \"macro f1\": macro_f1, \"micro precision\": micro_prec, \"micro recall\": micro_recall, \"micro f1\": micro_f1, \"weighted average precision\": wa_prec, \"weighted average recall\": wa_recall, \"weighted average f1\": wa_f1}\n",
    "    return (scores, output)\n",
    "\n",
    "#testing\n",
    "#evaluate(result, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Since this is a multiclass classification problem, there are multiple ways to compute precision, recall, and F-score for this classifier. Implement at least two of the methods from the \"Model Evaluation\" lecture and discuss any differences between them. (The implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running naive bayes\n",
    "\n",
    "train_df = preprocess(\"COMP30027_2021_assignment1_data/train.csv\")\n",
    "test_df = preprocess(\"COMP30027_2021_assignment1_data/test.csv\")\n",
    "\n",
    "result = predict(train_df, test_df)\n",
    "true_labels = list(test_df.label)\n",
    "overall, indiv = evaluate(result, true_labels)\n",
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "The Gaussian naıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in this dataset? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output for Q2\")\n",
    "fig, axs = plt.subplots(22, 3, figsize=(20, 100))\n",
    "\n",
    "n_bins = 20\n",
    "train_data = preprocess(TRAIN_LOCATION)\n",
    "classes = [\"tree\", \"bridge\"]\n",
    "colours = [\"royalblue\", \"maroon\"]\n",
    "\n",
    "for idx, target_class in enumerate(classes):\n",
    "    target = train_data.groupby(\"label\").get_group(target_class) \n",
    "    \n",
    "    for col_x, col_y, ax in zip(train_data.columns[1:12], train_data.columns[12:], axs[idx*11:]):\n",
    "        ax[0].hist(x=target[col_x].dropna(), bins = n_bins, color = colours[idx], ec=colours[idx])\n",
    "        ax[0].set_title(\"{} Histogram for class {}\".format(col_x, target_class))\n",
    "        ax[1].hist(x=target[col_y].dropna(), bins = n_bins, color = colours[idx], ec=colours[idx])\n",
    "        ax[1].set_title(\"{} Histogram for class {}\".format(col_y, target_class))\n",
    "        ax[2].scatter(x=target[col_x], y=target[col_y], color = colours[idx], ec=colours[idx])\n",
    "        ax[2].set_title(\"({}, {}) Scatter Plot for class {}\".format(col_x, col_y,  target_class))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Implement a kernel density estimate (KDE) naive Bayes classifier and compare its performance to the Gaussian naive Bayes classifier. Recall that KDE has kernel bandwidth as a free parameter -- you can choose an arbitrary value for this, but a value in the range 5-25 is recommended. Discuss any differences you observe between the Gaussian and KDE naive Bayes classifiers. (As with the Gaussian naive Bayes, this KDE naive Bayes implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Instead of using an arbitrary kernel bandwidth for the KDE naive Bayes classifier, use random hold-out or cross-validation to choose the kernel bandwidth. Discuss how this changes the model performance compared to using an arbitrary kernel bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Naive Bayes ignores missing values, but in pose recognition tasks the missing values can be informative. Missing values indicate that some part of the body was obscured and sometimes this is relevant to the pose (e.g., holding one hand behind the back). Are missing values useful for this task? Implement a method that incorporates information about missing values and demonstrate whether it changes the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_missing(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    grouped = train.groupby(\"label\")\n",
    "    # Create empty dataframe with freatures in columns\n",
    "    missing_probabilities = pd.DataFrame(columns=train_df.columns[1:])\n",
    "\n",
    "    # Calculat probability of missing points within each label\n",
    "    for label in grouped.groups:\n",
    "        label_df = grouped.get_group(label)\n",
    "        row = (label_df.isna().sum() / len(label_df.index))[1:]\n",
    "        row.name = label\n",
    "        missing_probabilities = missing_probabilities.append(row)\n",
    "\n",
    "    return missing_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"output for Q5\")\n",
    "# run evaluation of predict() with missing value information incorporated \n",
    "result = predict(train_df, test_df, use_missing=True)\n",
    "true_labels = list(test_df.label)\n",
    "overall_with_missing, indiv_with_missing = evaluate(result, true_labels)\n",
    "overall_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in classification results\n",
    "(indiv_with_missing - indiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding percentage  of missing value in each pose\n",
    "train_grouped = train_df.groupby(\"label\")\n",
    "test_grouped = test_df.groupby(\"label\")\n",
    "train_missing_ratio = train_grouped.count().rsub(train_grouped.size(), axis=0).sum(axis=1) / (train_grouped.size() * len(train_df.columns[1:]))\n",
    "test_missing_ratio = test_grouped.count().rsub(test_grouped.size(), axis=0).sum(axis=1) / (test_grouped.size() * len(test_df.columns[1:]))\n",
    "summary = pd.DataFrame({\"train\":train_missing_ratio, \"test\":test_missing_ratio, \"# of train\": train_grouped.size(), \"# of test\": test_grouped.size()})\n",
    "summary\n"
   ]
  },
  {
   "source": [
    "### Q6\n",
    "Engineer your own pose features from the provided keypoints. Instead of using the (x,y) positions of keypoints, you might consider the angles of the limbs or body, or the distances between pairs of keypoints. How does a naive Bayes classifier based on your engineered features compare to the classifier using (x,y) values? Please note that we are interested in explainable features for pose recognition, so simply putting the (x,y) values in a neural network or similar to get an arbitrary embedding will not receive full credit for this question. You should be able to explain the rationale behind your proposed features. Also, don't forget the conditional independence assumption of naive Bayes when proposing new features -- a large set of highly-correlated features may not work well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0db1e82c7aa95844e45fbbbbcd794a7c1e0efcba29fed4bfa108b76bffa77ce80",
   "display_name": "Python 3.8.5 64-bit ('3.8.5')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "0b59cb172bf3abc9eab5703b35732b5fd55c7abcc81d6269694b07225b46ddda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
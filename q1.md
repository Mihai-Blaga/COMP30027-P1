**Question 1**

Nine different techniques for calculating scores are evaluated and outputted in the `evaluate()` function, but for this question we will focus on *macro precision* (MP) and *weighted average precision* (WAP).

The critical difference between MP and WAP is in the weight each class is given for the final calculation. In MP, each class is treated equally, so each class' precision contributes an equal amount to the overall MP calculation. However, in WAP, as the name implies, each class contributes an amount proportional to its occurence in the test set.

An assumption made when choosing the test and training data is that both sets of data are representative of the real world. As such, it is expected that both sets will have similar distributions of class labels, i.e. a class which is rarely seen in the training data is also rarely seen in the test data.
Further, it is generally true that the systems *learns* more about labels that are seen more frequently. As such, it is expected that frequently occuring labels will score higher than labels which are not seen as frequently.

As MP treats all classes equally, this expected correlation between frequency and score is lost while in WAP, it impacts the final outcome. As seen in the table labelled "Output table for Q1", if the data is seperated into two halves, the higher frequency half collectively occurs 74 times while the lower frequency half occurs 42 times. 